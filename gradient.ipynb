{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28c1ab49-ce76-4417-9e14-9bc3ff84992d",
   "metadata": {},
   "source": [
    "# Gradient Descent Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0bd4b9-76bc-4e0e-88c8-838e44c25dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline as imbpipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import make_scorer, accuracy_score,precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "from imblearn.over_sampling._smote.base import SMOTE\n",
    "import missingno as msno\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5560e5a-2d7a-4507-b30e-5ba7b087255c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Importing necessary libraries and packages.\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.config.optimizer.set_jit(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a4e7bd-5c29-45b4-a2c6-3529dd3ba881",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualization settings\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "sns.set_style(\"whitegrid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7aca8b-7eea-41a7-be46-34b23cd0ac33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Pandas Setting\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.set_option('display.width', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17df7428-6388-447b-835e-49a9ca790c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "datam= pd.read_csv('/Users/erkan/Downloads/feature_engineering/feature_engineering/datasets/report_2018-2019.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2cba13-ff6f-409c-8e14-f1f05f9a3eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "datam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d133c991-a6d0-47c2-abde-1e743c4367e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6ba03d-d5c6-4f58-9bdf-e27ec08e683c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d37966-e61b-4c17-898e-6ba50f9f54b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols, num_cols, cat_but_car = grab_col_names(datam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae765d55-4e39-4c07-b689-9c3454978f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in num_cols:\n",
    "    sns.boxplot(x=col, data=datam)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53944435-31e5-4227-8894-13f343d4e42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "datam['Country or region'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d83355-cf28-4599-933d-3cf8e4f9cef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "datam.drop(['Country or region', 'Overall rank'], axis=1, inplace=True)\n",
    "datam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cde069-27ca-4b8a-9f33-76051f863180",
   "metadata": {},
   "outputs": [],
   "source": [
    "datam = pd.get_dummies(datam, columns=['Year'], drop_first=True)\n",
    "datam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a91c640-1cf6-4e00-a202-e8986b9215a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = datam.drop('Score', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e605bf5-469a-402d-8eaf-afa0cedb97a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = datam['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830d3088-5f24-4b9f-9355-7a724f45aac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d221ca86-134c-4284-acb3-d00dc6cb8359",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06524af2-085b-4ac5-a607-218bbee24c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_b = np.c_[np.ones((X_train.shape[0], 1)), X_train]\n",
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y_train)\n",
    "y_predict = X_b.dot(theta_best)\n",
    "theta_path_bgd = []\n",
    "np.random.seed(42)\n",
    "theta = np.random.randn(8,1)\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "\n",
    "\n",
    "def plot_gradient_descent(theta, eta, theta_path=None):\n",
    "    m = len(X_b)\n",
    "    plt.plot(X_train, y_train, \"b.\")\n",
    "    n_iterations = 1000\n",
    "    for iteration in range(n_iterations):\n",
    "        if iteration < 10:\n",
    "            y_predict = X_b.dot(theta)\n",
    "            style = \"b-\" if iteration > 0 else \"r--\"\n",
    "            plt.plot(X_train, y_predict, style)\n",
    "        gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y_train)\n",
    "        theta = theta - eta * gradients\n",
    "        if theta_path is not None:\n",
    "            theta_path.append(theta)\n",
    "    plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "    plt.axis([0, 2, 0, 15])\n",
    "    plt.title(r\"$\\eta = {}$\".format(eta), fontsize=16)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(131); plot_gradient_descent(theta, eta=0.01)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.subplot(132); plot_gradient_descent(theta, eta=0.1, theta_path=theta_path_bgd)\n",
    "plt.subplot(133); plot_gradient_descent(theta, eta=1)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37633fd8-fbc7-4db7-8902-47c8e7fac04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the cost (Mean Squared Error)\n",
    "\n",
    "def compute_cost(X, y, theta):\n",
    "    m = len(X)\n",
    "    predictions = X.dot(theta)\n",
    "    cost = (1 / (2 * m)) * np.sum((predictions - y) ** 2)\n",
    "    return cost\n",
    "\n",
    "# Gradient Descent implementation with convergence detection\n",
    "\n",
    "def gradient_descent(X, y, theta, eta, n_iterations, tol=1e-6):\n",
    "    m = len(X)\n",
    "    cost_history = []  \n",
    "    for iteration in range(n_iterations):\n",
    "        predictions = X.dot(theta)\n",
    "        gradients = (2 / m) * X.T.dot(predictions - y)\n",
    "        theta = theta - eta * gradients\n",
    "        \n",
    "        # Save the cost at each iteration for plotting\n",
    "        cost = compute_cost(X, y, theta)\n",
    "        cost_history.append(cost)\n",
    "        \n",
    "        # Check for convergence (when the change in cost is less than tolerance)\n",
    "        if iteration > 0 and abs(cost_history[-1] - cost_history[-2]) < tol:\n",
    "            return theta, cost_history, iteration + 1  # Return the iteration count (1-based)\n",
    "    \n",
    "    return theta, cost_history, n_iterations \n",
    "# Initialize data\n",
    "X_b = np.c_[np.ones((X_train.shape[0], 1)), X_train]  # Add the bias term (1's column)\n",
    "y_train = y_train.reshape(-1, 1)  # Ensure y_train is a column vector\n",
    "theta = np.random.randn(X_b.shape[1], 1)  # Initialize theta randomly\n",
    "\n",
    "# Run Gradient Descent\n",
    "eta = 0.01  # Learning rate\n",
    "n_iterations = 1000\n",
    "theta_best, cost_history, n_iterations = gradient_descent(X_b, y_train, theta, eta, n_iterations)\n",
    "\n",
    "# Plotting the cost function over iterations\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.arange(n_iterations), cost_history, label=f\"Learning rate = {eta}\")\n",
    "plt.xlabel(\"Number of iterations\")\n",
    "plt.ylabel(\"Cost (MSE)\")\n",
    "plt.title(\"Convergence of Gradient Descent\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7415d44b-a6ce-40f0-89b6-6b82762fac91",
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.1  # Learning rate\n",
    "n_iterations = 1000\n",
    "theta_best, cost_history, n_iterations = gradient_descent(X_b, y_train, theta, eta, n_iterations)\n",
    "\n",
    "# Plotting the cost function over iterations\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.arange(n_iterations), cost_history, label=f\"Learning rate = {eta}\")\n",
    "plt.xlabel(\"Number of iterations\")\n",
    "plt.ylabel(\"Cost (MSE)\")\n",
    "plt.title(\"Convergence of Gradient Descent\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4546d8b4-0a52-4c3b-acaf-4a59914f8926",
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 1  # Learning rate\n",
    "n_iterations = 1000\n",
    "theta_best, cost_history, n_iterations = gradient_descent(X_b, y_train, theta, eta, n_iterations)\n",
    "\n",
    "# Plotting the cost function over iterations\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.arange(n_iterations), cost_history, label=f\"Learning rate = {eta}\")\n",
    "plt.xlabel(\"Number of iterations\")\n",
    "plt.ylabel(\"Cost (MSE)\")\n",
    "plt.title(\"Convergence of Gradient Descent\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae95d7e-9b1a-45c5-9304-dad1a930e3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data\n",
    "X_b = np.c_[np.ones((X_train.shape[0], 1)), X_train]  # Add the bias term (1's column)\n",
    "y_train = y_train.reshape(-1, 1)  # Ensure y_train is a column vector\n",
    "\n",
    "# Learning rates to test\n",
    "learning_rates = [0.01, 0.1, 1]\n",
    "n_iterations = 1000\n",
    "results = {}\n",
    "\n",
    "# Run Gradient Descent for each learning rate and track the number of iterations to converge\n",
    "for eta in learning_rates:\n",
    "    theta_initial = np.random.randn(X_b.shape[1], 1)  # Reinitialize theta for each case\n",
    "    theta_best, cost_history, n_iter = gradient_descent(X_b, y_train, theta_initial, eta, n_iterations)\n",
    "    results[eta] = n_iter\n",
    "    print(f\"Learning rate {eta} took {n_iter} iterations to converge.\")\n",
    "    \n",
    "    # Plot the cost function over iterations\n",
    "    plt.plot(np.arange(len(cost_history)), cost_history, label=f\"eta = {eta}\")\n",
    "\n",
    "# Plot the cost function for different learning rates\n",
    "plt.xlabel(\"Number of iterations\")\n",
    "plt.ylabel(\"Cost (MSE)\")\n",
    "plt.title(\"Convergence of Gradient Descent for Different Learning Rates\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "## Learning rate of 1 looks like overshoot. \n",
    "## Learning rate of 0.1 looks like very slow convergence.\n",
    "## Best learnin rate is 0.1 out of three.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
